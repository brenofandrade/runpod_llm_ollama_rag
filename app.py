# from langchain_community.document_loaders import PDFPlumberLoader
# from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_core.vectorstores import InMemoryVectorStore
# from langchain_ollama import OllamaEmbeddings
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_ollama.llms import OllamaLLM



from helper import *

PDF_STORAGE_PATH = 'data/'
# EMBEDDING_MODEL = OllamaEmbeddings(model="deepseek-r1:latest")
# DOCUMENT_VECTOR_DB = InMemoryVectorStore(EMBEDDING_MODEL)
# LANGUAGE_MODEL = OllamaLLM(model="deepseek-r1:latest")



# 



# DOCUMENT_VECTOR_DB.add_documents(document_chunks)
    



### UI Config


# import streamlit as st


# st.set_page_config(
#     page_title="Unimed",
#     page_icon="ü§ñ",
#     layout="wide"
# )

# st.title("Chat Unimed üìò")
# st.write("Assitente de intelig√™ncia para documentos!")
# st.markdown("---")


# with st.sidebar:

#     uploaded_pdf = st.file_uploader(
#         label="Selecione um documento (PDF)",
#         type="pdf",
#         accept_multiple_files=False
#     )


import streamlit as st

st.set_page_config(
    page_title="Unimed Chat RAG",
    page_icon="ü§ñ",
    layout="wide"
)

# -------------------- T√çTULO E DESCRI√á√ÉO --------------------
st.title("üìò Chat Unimed - Assistente de Documentos")
st.caption("Um assistente inteligente para responder perguntas com base em documentos da Unimed.")

st.markdown("---")

# -------------------- LAYOUT PRINCIPAL --------------------
col1 = st.columns([1], gap="large")

# Upload de PDF e par√¢metros na coluna lateral
with st.sidebar:
    st.header("üìÇ Documento")
    uploaded_pdf = st.file_uploader(
        label="Envie um documento PDF",
        type="pdf",
        accept_multiple_files=False
    )

    st.markdown("---")
    st.header("‚öôÔ∏è Par√¢metros do Modelo")
    temperature = st.slider("Temperatura", 0.0, 1.0, 0.2, 0.1)
    max_tokens = st.slider("M√°ximo de Tokens", 100, 1000, 500, 50)

    st.markdown("---")
    if uploaded_pdf:
        st.success("Documento carregado com sucesso!")
    else:
        st.info("Aguardando envio de documento...")

# Campo de chat e resposta

st.subheader("üí¨ Converse com o assistente")

user_question = st.text_input("Digite sua pergunta:", placeholder="Ex: Quais s√£o as coberturas do plano?")

if st.button("Perguntar"):
    if uploaded_pdf and user_question:
        st.info("üîç Processando sua pergunta com RAG...")
        # Aqui entraria a chamada para o backend RAG com embeddings
        # resposta = seu_modelo_rag(resposta)
        st.success("‚úÖ Resposta gerada com sucesso!")
        st.write("**Resposta:** Aqui est√° a resposta simulada baseada no documento.")
    else:
        st.warning("Por favor, envie um documento e digite uma pergunta.")






# Rodap√©
st.markdown("---")
st.caption("Desenvolvido com ‚ù§Ô∏è pela equipe de dados da Unimed")
